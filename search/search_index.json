{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Posts","text":""},{"location":"series/","title":"Series","text":""},{"location":"series/#tag:series-github-repo-issues-dataset","title":"Series: GitHub repo issues dataset","text":"<ul> <li>            Getting GitHub repo issues and comments          </li> <li>            Making a Hugging Face dataset of GitHub repo issues          </li> <li>            Retrieving information from a GitHub repo issues dataset          </li> </ul>"},{"location":"tools/","title":"Tools","text":""},{"location":"tools/#tag:hugging-face","title":"Hugging Face","text":"<ul> <li>            Cleaning UCI ML Drug Review Dataset          </li> <li>            Making a Hugging Face dataset of GitHub repo issues          </li> <li>            Retrieving information from a GitHub repo issues dataset          </li> </ul>"},{"location":"tools/#tag:material-for-mkdocs","title":"Material for MkDocs","text":"<ul> <li>            Setting up a blog with Material for MkDocs          </li> </ul>"},{"location":"tools/#tag:polars","title":"Polars","text":"<ul> <li>            Cleaning UCI ML Drug Review Dataset          </li> <li>            Making a Hugging Face dataset of GitHub repo issues          </li> <li>            Retrieving information from a GitHub repo issues dataset          </li> <li>            Stratified train-test split with Polars          </li> </ul>"},{"location":"tools/#tag:rest-api","title":"REST API","text":"<ul> <li>            Getting GitHub repo issues and comments          </li> </ul>"},{"location":"topics/","title":"Topics","text":""},{"location":"topics/#tag:nlp","title":"NLP","text":"<ul> <li>            Retrieving information from a GitHub repo issues dataset          </li> </ul>"},{"location":"clean-uci-drug-review-dataset/","title":"Cleaning UCI ML Drug Review Dataset","text":"<p>This is my  Polars-centric adaptation of the tutorial  Hugging Face NLP Course: Time to slice and dice, which cleans the UCI ML Drug Review dataset available on  Kaggle.</p> <p>Cleaned dataset on Hugging Face Hub:  dd-n-kk/uci-drug-review-cleaned</p> <p>  Colab notebook </p>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#preparations","title":"Preparations","text":"<pre><code># Set this to an empty string to avoid saving the dataset file.\nDATA_DIR = \"uci-drug-review-cleaned/\"\n\n# Set these to empty strings to avoid uploading the dataset.\nREPO_ID = \"dd-n-kk/uci-drug-review-cleaned\"\nSECRET = \"HF_TOKEN\"\n</code></pre> <pre><code>!uv pip install --system -Uq polars\n</code></pre> <pre><code>import kagglehub\nimport polars as pl\nfrom polars import col\n</code></pre> <pre><code>path = kagglehub.dataset_download(\"jessicali9530/kuc-hackathon-winter-2018\")\n!mkdir data/ &amp;&amp; cp -t data/ {path}/* &amp;&amp; ls -hAil data\n</code></pre> <pre><code>Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/kuc-hackathon-winter-2018?dataset_version_number=2...\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40.7M/40.7M [00:00&lt;00:00, 44.6MB/s]\n\nExtracting files...\n\n\n\n\n\ntotal 106M\n2364654 -rw-r--r-- 1 root root 27M Mar 22 13:17 drugsComTest_raw.csv\n2364655 -rw-r--r-- 1 root root 80M Mar 22 13:17 drugsComTrain_raw.csv\n</code></pre> <pre><code>SEED = 777\npl.set_random_seed(SEED)\n\n# Configure Polars for more complete display.\n_ = pl.Config(\n    tbl_cols=-1,\n    tbl_rows=100,\n    tbl_width_chars=-1,\n    float_precision=3,\n    fmt_str_lengths=500,\n    fmt_table_cell_list_len=-1,\n)\n</code></pre> <pre><code>df = pl.read_csv(\"data/drugsComTrain_raw.csv\")\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#observations","title":"Observations","text":"<ul> <li> <p><code>drugName</code>:</p> <ul> <li>Capitalization is inconsistent (\"A + D Cracked Skin Relief\" vs. \"femhrt\").</li> <li>There are combination prescriptions (\"Ethinyl estradiol / norgestimate\").</li> </ul> </li> <li> <p><code>condition</code>:</p> <ul> <li>There are 899 nulls.</li> <li>There are corrupted values:<ul> <li>\"0<code>&lt;/span&gt;</code> users found this comment helpful.\"</li> <li>\"zen Shoulde\" should probably be \"Frozen Shoulder\".</li> </ul> </li> </ul> </li> <li> <p><code>review</code>:</p> <ul> <li>There are HTML entities (<code>&amp;#039;</code>).</li> <li>There are emojis (\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f).</li> </ul> </li> <li> <p><code>date</code>:</p> <ul> <li>The format could be simplified (\"1-Apr-08\").</li> </ul> </li> </ul> <pre><code>df.describe()\n</code></pre> shape: (9, 8)statisticuniqueIDdrugNameconditionreviewratingdateusefulCountstrf64strstrstrf64strf64\"count\"161297.000\"161297\"\"160398\"\"161297\"161297.000\"161297\"161297.000\"null_count\"0.000\"0\"\"899\"\"0\"0.000\"0\"0.000\"mean\"115923.585nullnullnull6.994null28.005\"std\"67004.445nullnullnull3.272null36.404\"min\"2.000\"A + D Cracked Skin Relief\"\"0&lt;/span&gt; users found this comment helpful.\"\"\"   please tell the ones who is suffering from anxiety to use lavender chamomile spray by air wick.\u00a0\u00a0it gives immediate relief , doctors not letting know patients about this. please spread the word!!.\u00a0\u00a0Please keep this post here.\"\"1.000\"1-Apr-08\"0.000\"25%\"58063.000nullnullnull5.000null6.000\"50%\"115744.000nullnullnull8.000null16.000\"75%\"173776.000nullnullnull10.000null36.000\"max\"232291.000\"femhrt\"\"zen Shoulde\"\"\"\u2764\ufe0f\u2764\ufe0f\u2764\ufe0f Cialis for US!!\u00a0\u00a0\u00a0\u00a0I wish I had my husband start this decades ago \"\"10.000\"9-Sep-17\"1291.000 <pre><code>df.sample(10, seed=SEED)\n</code></pre> shape: (10, 7)uniqueIDdrugNameconditionreviewratingdateusefulCounti64strstrstri64stri6463419\"Epiduo\"\"Acne\"\"\"I have been using epiduo for almost a week now and it has been burning a little bit, so i cut back on how much i put on my face and it feels much better. I did not have alot of acne at all before using it but only a few on my cheeks and those are all gone now after a few days of use. The few bumps on ny forehead are still there but im positive the epiduo is fighting to get rid of it. I will give it a couple of weeks :). (avoid using in eye area, and area right AROUND nose... those are sensitive\u20268\"27-Jun-16\"4152744\"Doxycycline\"\"Bacterial Infection\"\"\"Have been taking this med now for the past 8 days, for an acute sinusitis infection, and paid strict attention to the pharmacists instructions..... YOU MUST DRINK A FULL GLASS OF WATER after you&amp;#039;ve taken it.\u00a0\u00a0Don&amp;#039;t disregard these rules, or you will have severe side effects.\u00a0\u00a0\u00a0Otherwise I&amp;#039;m hoping this time, doxy will clear it up as I was on amoxy and that didn&amp;#039;t. This infection keeps repeating because of two bouts of pneumonia, and I&amp;#039;m sure residual infection wasn&amp;#039\u20268\"3-Nov-17\"2126485\"Brimonidine\"\"Rosacea\"\"\"Stay away from this medication. The 1st day used, I was impressed, but 2 days later I had a rebound and my face was burning like hell. I waited like 4 days to re-apply and see what happens, but I lost sensation on my lips and they got swollen. Then, after 24 hours the rebound got worse and was like two weeks after my skin stopped burning. I still don&amp;#039;t understand how this product was approved by the FDA.\"\"1\"27-Jun-16\"21208405\"Oseltamivir\"\"Influenza\"\"\"Sick sick with flu, tamiflu added to illness, nausea &amp;amp; vomiting, don&amp;#039;t know if made flu last shorter, made me too nauseous to know, just ride out the virus, plan on being sick for a week, don&amp;#039;t burden your body with meds that may make you sick\"\"2\"25-Dec-15\"17166105\"Levonorgestrel\"null\"\"Well I&amp;#039;m the first one reviewing this... I got my Kyleena IUD placed on Dec. 2nd 2016. It was painful but no more so than other IUDs I&amp;#039;ve read about. The issues I&amp;#039;m having is contant spotting. I&amp;#039;ve been bleeding lightly (where I have to wear a light pad so I don&amp;#039;t ruin my underwear every day) for two months now. The office told me about a month ago that my spotting is normal and most likely means once I stop spotting I&amp;#039;ll be done with my period for the five years K\u20265\"30-Jan-17\"1716075\"Ethinyl estradiol / norethindrone\"\"Birth Control\"\"\"I&amp;#039;m 14 years old and I&amp;#039;ve been on it for 4 months and I hate it. I&amp;#039;m on it for regular periods since my blood flows too quickly and it has done nothing. I&amp;#039;ve been on my period for 4 months, light mostly but heavy sometimes. I also went on it for my acne but it did nothing. I gained 4 pounds and now I have stubborn belly fat. I also have huge stretch marks (idk if its related but they showed up when I took it). Also, I have no motivation anymore. Don&amp;#039;t take this please!\"\"3\"27-May-16\"4223196\"Estradiol\"\"Postmenopausal Symptoms\"\"\"I have had a great experience with the Minivelle patch.\u00a0\u00a0I was suffering with many, many, hot flashes a day and night sweats that left me drenched!\u00a0\u00a0I am down to a handful of flashes a day and the night sweats are gone!\"\"10\"15-Jul-13\"41102766\"Aripiprazole\"\"Major Depressive Disorde\"\"\"Weight gain and fatigue.\"\"3\"29-Jul-14\"1585534\"Ethinyl estradiol / norgestimate\"\"Birth Control\"\"\"This Is An Amazing Birth Control. I&amp;#039;ve Been On This For Five Years And Counting. I Never Got Pregnant, My Menstrual Last About 3-4 Days(Regular Period ) ,No Cramps, Face Is Clear Of Acne, From Time To Time Ive Gotten Moody But Overall I Love It!\u00a0\u00a0I&amp;#039;m Looking Forward To Getting Off And Starting A Family.\"\"9\"17-Feb-15\"24131566\"Methyl salicylate\"\"Muscle Pain\"\"\"The adhesive used for these particular patches works a lot better than the other comparable products on the market by far. As long as you get them on straight and unwrinkled the first time, they will stay on for virtually as long as they last, which also happens to exceed comparable products by far. Couple this with the much less expensive price and you have three reasons why this is simply a superior product to the others.   It should be noted that the one time I almost bought another name b\u20267\"7-Jun-10\"36","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#verifying-uniqueness-of-uniqueid","title":"Verifying uniqueness of <code>uniqueID</code>","text":"<p>Unlike the version used in the Hugging Face tutorial, the Kaggle-hosted dataset does have a name for the first column. So here we just verify that each row indeed has a unique ID.</p> <pre><code>assert df.get_column(\"uniqueID\").n_unique() == len(df)\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#scanning-for-invalid-drugnames","title":"Scanning for invalid <code>drugName</code>s","text":"<p>We scan for invalid drug names by looking for unusual characters. Fortunately most if not all drug names seem to be valid.</p> <pre><code>(\n    df.filter(col(\"drugName\").str.contains(r\"[^[:alnum:] '(),./-]\"))\n    .get_column(\"drugName\")\n    .value_counts()\n    .sort(\"count\", descending=True)\n)\n</code></pre> shape: (17, 2)drugNamecountstru32\"Tylenol with Codeine #3\"65\"Coricidin HBP Cold &amp; Flu\"5\"Tylenol with Codeine #4\"4\"Mucinex Fast-Max Severe Congestion &amp; Cough\"2\"Dimetapp Children's Cold &amp; Cough\"2\"Aleve-D Sinus &amp; Cold\"1\"Coricidin HBP Cough &amp; Cold\"1\"Sudafed PE Pressure + Pain\"1\"A + D Cracked Skin Relief\"1\"Norinyl 1+35\"1\"PanOxyl 10% Acne Foaming Wash\"1\"Hair, Skin &amp; Nails\"1\"Vicks Dayquil Cold &amp; Flu Relief\"1\"Robitussin Cough + Chest Congestion DM\"1\"Excedrin Back &amp; Body\"1\"Mucinex Fast-Max Night Time Cold &amp; Flu\"1\"Vitafol-OB+DHA\"1","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#handling-capitalization-of-drugnames","title":"Handling capitalization of <code>drugName</code>s","text":"<p>There are very few lowercase drug names and all of them are valid. We can also confirm that no drug name is cased differently in the dataset. To preserve the original data as much as possible, I title-case the lowercase names instead of lowercasing all drug names.</p> <pre><code>(\n    df.filter(col(\"drugName\").str.contains(r\"^[^A-Z]\"))\n    .get_column(\"drugName\")\n    .value_counts()\n    .sort(\"count\", descending=True)\n)\n</code></pre> shape: (3, 2)drugNamecountstru32\"ella\"51\"femhrt\"3\"depo-subQ provera 104\"1 <pre><code>assert (\n    df.get_column(\"drugName\").n_unique()\n    == df.get_column(\"drugName\").str.to_lowercase().n_unique()\n)\n</code></pre> <pre><code>replacements = {\n    \"ella\": \"Ella\",\n    \"femhrt\": \"Femhrt\",\n    \"depo-subQ provera 104\": \"Depo-SubQ Provera 104\"\n}\n\ndf = df.with_columns(col(\"drugName\").replace(replacements))\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#cleaning-invalid-conditions","title":"Cleaning invalid <code>condition</code>s","text":"<p>The search for unusual characters yields a group of rows with invalid <code>condition</code>s: \"??<code>&lt;/span&gt;</code> users found this comment helpful.\" I decide to replace them with nulls.</p> <pre><code>q = df.filter(col(\"condition\").str.contains(r\"[^[:alnum:] '(),./-]\")); len(q)\n</code></pre> <pre><code>900\n</code></pre> <pre><code>q.select(pl.exclude(\"review\")).sample(10, seed=SEED)\n</code></pre> shape: (10, 6)uniqueIDdrugNameconditionratingdateusefulCounti64strstri64stri6481207\"Yaz\"\"0&lt;/span&gt; users found this comment helpful.\"1\"11-Feb-17\"0126293\"Viibryd\"\"15&lt;/span&gt; users found this comment helpful.\"5\"14-Oct-11\"1524826\"Deplin\"\"13&lt;/span&gt; users found this comment helpful.\"6\"9-Aug-16\"1367383\"Provera\"\"4&lt;/span&gt; users found this comment helpful.\"1\"27-Mar-16\"4212726\"Phenergan\"\"3&lt;/span&gt; users found this comment helpful.\"10\"3-Jan-09\"3220738\"Loestrin 24 Fe\"\"9&lt;/span&gt; users found this comment helpful.\"5\"13-Aug-10\"933084\"Seasonique\"\"2&lt;/span&gt; users found this comment helpful.\"8\"29-Jun-10\"2159396\"Estrace Vaginal Cream\"\"64&lt;/span&gt; users found this comment helpful.\"8\"14-Sep-11\"64209258\"Geodon\"\"5&lt;/span&gt; users found this comment helpful.\"6\"18-Feb-09\"571470\"Levora\"\"2&lt;/span&gt; users found this comment helpful.\"3\"15-Sep-11\"2 <pre><code>df = df.with_columns(\n    pl.when(col(\"condition\").str.contains(\"&lt;/span&gt;\", literal=True))\n    .then(pl.lit(None))\n    .otherwise(col(\"condition\"))\n    .alias(\"condition\")\n)\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#correcting-corrupted-conditions","title":"Correcting corrupted <code>condition</code>s","text":"<p>The search for lowercase condition names leads to a few unexpected findings:</p> <ul> <li>Some <code>condition</code> fields mistakenly record drug names instead of condition names.</li> <li>Quite a few condition names lost their prefixes, suffixes, or both.   Indeed, seemingly all leading <code>F</code>s and terminating <code>r</code>s were cut off.</li> </ul> <p>I decide to handle the corrupted entries one by one: Correct those I can recognize and nullify those I cannot.</p> <pre><code>(\n    df.filter(col(\"condition\").str.contains(r\"^[^A-Z]\"))\n    .get_column(\"condition\")\n    .value_counts()\n    .sort(\"count\", descending=True)\n)\n</code></pre> shape: (37, 2)conditioncountstru32\"ibromyalgia\"1791\"mance Anxiety\"187\"moterol)\"79\"emale Infertility\"65\"atigue\"53\"min)\"45\"ge (amlodipine / valsartan)\"33\"acial Wrinkles\"32\"moterol / mometasone)\"29\"min / sitagliptin)\"26\"zen Shoulde\"14\"eve\"12\"mulation) (phenylephrine)\"12\"min / saxagliptin)\"10\"lic Acid Deficiency\"8\"von Willebrand's Disease\"7\"amilial Mediterranean Feve\"6\"min / pioglitazone)\"5\"mis\"5\"ge HCT (amlodipine / hydrochlorothiazide / valsartan)\"4\"cal Segmental Glomerulosclerosis\"4\"ailure to Thrive\"3\"amilial Cold Autoinflammatory Syndrome\"3\"actor IX Deficiency\"3\"t Pac with Cyclobenzaprine (cyclobenzaprine)\"2\"t Care\"2\"min / rosiglitazone)\"2\"tic (mycophenolic acid)\"2\"llicular Lymphoma\"1\"ungal Pneumonia\"1\"mist (\"1\"m Pain Disorde\"1\"me\"1\"unctional Gastric Disorde\"1\"acial Lipoatrophy\"1\"ibrocystic Breast Disease\"1\"ungal Infection Prophylaxis\"1 <pre><code>replacements = {\n    \"ibromyalgia\": \"Fibromyalgia\",\n    \"mance Anxiety\": \"Performance Anxiety\",\n    \"emale Infertility\": \"Female Infertility\",\n    \"atigue\": \"Fatigue\",\n    \"acial Wrinkles\": \"Facial Wrinkles\",\n    \"zen Shoulde\": \"Frozen Shoulder\",\n    \"eve\": \"Fever\",\n    \"lic Acid Deficiency\": \"Folic Acid Deficiency\",\n    \"von Willebrand's Disease\": \"Von Willebrand's Disease\",\n    \"amilial Mediterranean Feve\": \"Familial Mediterranean Fever\",\n    \"cal Segmental Glomerulosclerosis\": \"Focal Segmental Glomerulosclerosis\",\n    \"ailure to Thrive\": \"Failure to Thrive\",\n    \"amilial Cold Autoinflammatory Syndrome\": \"Familial Cold Autoinflammatory Syndrome\",\n    \"actor IX Deficiency\": \"Factor IX Deficiency\",\n    \"acial Lipoatrophy\": \"Facial Lipoatrophy\",\n    \"ungal Pneumonia\": \"Fungal Pneumonia\",\n    \"llicular Lymphoma\": \"Follicular Lymphoma\",\n    \"unctional Gastric Disorde\": \"Functional Gastric Disorder\",\n    \"ungal Infection Prophylaxis\": \"Fungal Infection Prophylaxis\",\n    \"ibrocystic Breast Disease\": \"Fibrocystic Breast Disease\",\n    \"llicle Stimulation\": \"Follicle Stimulation\",\n\n    \"moterol)\": None,\n    \"min)\": None,\n    \"ge (amlodipine / valsartan)\": None,\n    \"moterol / mometasone)\": None,\n    \"min / sitagliptin)\": None,\n    \"mulation) (phenylephrine)\": None,\n    \"min / saxagliptin)\": None,\n    \"mis\": None,\n    \"min / pioglitazone)\": None,\n    \"ge HCT (amlodipine / hydrochlorothiazide / valsartan)\": None,\n    \"min / rosiglitazone)\": None,\n    \"tic (mycophenolic acid)\": None,\n    \"t Care\": None,\n    \"t Pac with Cyclobenzaprine (cyclobenzaprine)\": None,\n    \"me\": None,\n    \"mist (\": None,\n    \"m Pain Disorde\": None,\n}\n\ndf = df.with_columns(col(\"condition\").replace(replacements))\n</code></pre> <p>Identifying all conditions losing their terminating <code>r</code>s would require nontrivial domain knowledge and effort. I can only come up with and correct \"disorde\" and \"(f)eve\", which should be \"disorder\" and \"fever\", respectively.</p> <pre><code>(\n    df.filter(col(\"condition\").str.ends_with(\"eve\"))\n    .get_column(\"condition\")\n    .value_counts()\n)\n</code></pre> shape: (3, 2)conditioncountstru32\"Q Feve\"1\"Typhoid Feve\"2\"Pain/Feve\"13 <pre><code>(\n    df.filter(col(\"condition\").str.ends_with(\"rde\"))\n    .get_column(\"condition\")\n    .value_counts()\n)\n</code></pre> shape: (27, 2)conditioncountstru32\"Binge Eating Disorde\"72\"Temporomandibular Joint Disorde\"30\"Bleeding Disorde\"7\"Social Anxiety Disorde\"389\"Borderline Personality Disorde\"151\"Hypoactive Sexual Desire Disorde\"7\"Shift Work Sleep Disorde\"60\"Post Traumatic Stress Disorde\"314\"Somatoform Pain Disorde\"1\"Dissociative Identity Disorde\"1\"Bipolar Disorde\"4224\"Tic Disorde\"2\"Persistent Depressive Disorde\"12\"Major Depressive Disorde\"1607\"Schizoaffective Disorde\"396\"Paranoid Disorde\"38\"Cyclothymic Disorde\"5\"Auditory Processing Disorde\"5\"Seasonal Affective Disorde\"16\"Periodic Limb Movement Disorde\"43\"Panic Disorde\"1463\"Intermittent Explosive Disorde\"2\"Generalized Anxiety Disorde\"1164\"Oppositional Defiant Disorde\"1\"Obsessive Compulsive Disorde\"579\"Body Dysmorphic Disorde\"9\"Premenstrual Dysphoric Disorde\"298 <pre><code>df = df.with_columns(\n    col(\"condition\").str.replace(r\"^(.+(?:rd|ev)e)$\", \"${1}r\").alias(\"condition\")\n)\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#decoding-html-entities-in-reviews","title":"Decoding HTML entities in <code>review</code>s","text":"<p>It turns out that many HTML entities besides <code>&amp;#039;</code> are present in the <code>review</code> column and using <code>html.unescape()</code> as recommended by the Hugging Face tutorial is probably more robust than replacing them individually. So we do just that.</p> <pre><code>q = (\n    df.select(col(\"review\").str.extract_all(r\"&amp;#?[[:alnum:]]+;\"))\n    .explode(\"review\")\n    .get_column(\"review\")\n    .value_counts()\n    .sort(\"count\", descending=True)\n); len(q)\n</code></pre> <pre><code>45\n</code></pre> <pre><code>q.head(10)\n</code></pre> shape: (10, 2)reviewcountstru32\"&amp;#039;\"262415null55984\"&amp;quot;\"21262\"&amp;amp;\"13047\"&amp;rsquo;\"3029\"&amp;gt;\"162\"&amp;rdquo;\"117\"&amp;ldquo;\"116\"&amp;eacute;\"111\"&amp;lt;\"108 <pre><code>import html\n\ndf = df.with_columns(col(\"review\").map_elements(html.unescape, return_dtype=pl.String))\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#simplifying-newlines-in-reviews","title":"Simplifying newlines in <code>review</code>s","text":"<p>Another issue of the <code>review</code>s pointed out by the Hugging Face tutorial is the presence of many different kinds of newline characters. I decide to replace all consecutive newline characters with one <code>\\n</code>.</p> <pre><code>df.filter(col(\"uniqueID\") == 121260).item(0, \"review\")\n</code></pre> <pre><code>'\"More than Depression I take Effexor for Anxiety and it works well.  The only problem\\r\\r\\noccurs when I forget to take it.  Within in hours I experience withdrawal symptoms\\r\\r\\nsuch as light headiness and an occasional brief buzzing in my head.\"'\n</code></pre> <pre><code>df = df.with_columns(col(\"review\").str.replace_all(r\"[\\r\\n]+\", \"\\n\"))\n</code></pre> <pre><code>df.filter(col(\"uniqueID\") == 121260).item(0, \"review\")\n</code></pre> <pre><code>'\"More than Depression I take Effexor for Anxiety and it works well.  The only problem\\noccurs when I forget to take it.  Within in hours I experience withdrawal symptoms\\nsuch as light headiness and an occasional brief buzzing in my head.\"'\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#reformatting-date","title":"Reformatting <code>date</code>","text":"<p>Finally, I reformat the <code>dates</code> into \"yyyy-mm-dd\", which is sortable as string.</p> <pre><code>df = df.with_columns(col(\"date\").str.to_date(\"%-d-%b-%y\"))\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"clean-uci-drug-review-dataset/#saving-and-sharing","title":"Saving and sharing","text":"<p>The test set is processed the same way and the procedure is omitted for brevity.</p> <pre><code>if DATA_DIR:\n    import os\n\n    os.makedirs(DATA_DIR, exist_ok=True)\n    df.write_csv(f\"{DATA_DIR}/train.tsv\", separator=\"\\t\")\n\nif REPO_ID and SECRET:\n    !uv pip install --system -q datasets\n\n    from datasets import load_dataset\n    from google.colab import userdata\n\n    dataset = load_dataset(\n        \"csv\",\n        data_files=dict(train=f\"{DATA_DIR}/train.tsv\", test=f\"{DATA_DIR}/test.tsv\"),\n        delimiter=\"\\t\",\n    )\n\n    dataset.push_to_hub(REPO_ID, token=userdata.get(SECRET))\n</code></pre> <pre><code>Uploading the dataset shards:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nCreating parquet from Arrow format:   0%|          | 0/162 [00:00&lt;?, ?ba/s]\n\n\n\nUploading the dataset shards:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nCreating parquet from Arrow format:   0%|          | 0/54 [00:00&lt;?, ?ba/s]\n</code></pre>","tags":["Polars","Hugging Face"]},{"location":"get-github-repo-issues-comments/","title":"Getting GitHub repo issues and comments","text":"<p>This is Part I of my adaptation of the tutorial  Hugging Face NLP Course: Creating your own dataset.</p> <p>  Colab notebook </p>","tags":["REST API","Series: GitHub repo issues dataset"]},{"location":"get-github-repo-issues-comments/#preparations","title":"Preparations","text":"<p>We use the  astral-sh/uv repository as an example.</p> <pre><code>from google.colab import userdata\n\nSECRET = \"GH_TOKEN\"\nREPO = \"astral-sh/uv\"\n</code></pre>","tags":["REST API","Series: GitHub repo issues dataset"]},{"location":"get-github-repo-issues-comments/#getting-paginated-data-from-github-rest-api","title":"Getting paginated data from GitHub REST API","text":"<p>The Hugging Face tutorial gets comments issue by issue, which is certainly viable but costs as many requests as the number of issues in the repository, which often surpasses even an authenticated quota of 5000 requests per hour.</p> <p>Alternatively, we can use the paginated repository issue comments endpoint to get up to 100 comments per request. This also allows getting issues and comments data in a unified approach.</p> <pre><code>import re\nimport sys\nfrom datetime import datetime\nfrom typing import Any, Literal\n\nimport requests\nfrom requests import Response\nfrom tqdm.auto import tqdm\n\n\ndef get_paginated(\n    route: str,\n    start_page: int = 1,\n    end_page: int | None = None,  # None: Make requests until the last page.\n    *,\n    token: str | None = None,\n    **query: Any,\n) -&gt; tuple[list[dict[str, Any]], Response]:\n    if start_page &lt; 1 or (end_page is not None and end_page &lt; start_page):\n        raise ValueError(f\"Invalid page range: [{start_page}, {end_page}]\")\n\n    headers = {\"Authorization\": f\"token {token}\"} if token else None\n    query |= {\"page\": start_page}\n    q = \"&amp;\".join(f\"{k}={v}\" for k, v in query.items() if v is not None)\n\n    results = []\n    with requests.Session() as session:\n        response = session.get(f\"https://api.github.com{route}?{q}\", headers=headers)\n        if response.status_code != 200:\n            print(f\"{response.status_code}: {response.reason}\", file=sys.stderr)\n            return results, response\n\n        results.extend(response.json())\n        links = parse_link_header(response)\n        end_page = _revise_end_page(start_page, end_page, response, links)\n        if end_page &lt;= start_page:\n            return results, response\n\n        with tqdm(total=end_page - start_page + 1) as progress:\n            progress.update(1)\n            while \"next\" in links:\n                response = session.get(links[\"next\"], headers=headers)\n                if response.status_code != 200:\n                    print(f\"{response.status_code}: {response.reason}\", file=sys.stderr)\n                    break\n\n                results.extend(response.json())\n                links = parse_link_header(response)\n                progress.update(1)\n\n    return results, response\n\n\ndef parse_link_header(response: Response) -&gt; dict[str, str]:\n    links = requests.utils.parse_header_links(response.headers[\"Link\"])\n    return {link[\"rel\"]: link[\"url\"] for link in links}\n\n\ndef _revise_end_page(\n    start_page: int, end_page: int | None, response: Response, links: dict[str, str]\n) -&gt; int:\n    if not (\"last\" in links and (m := re.search(r\"(?:[?&amp;]page=(\\d+))\", links[\"last\"]))):\n        return start_page\n\n    last = int(m.group(1))\n    limit = start_page + int(response.headers[\"X-RateLimit-Remaining\"])\n\n    return min(last, limit) if end_page is None else min(last, limit, end_page)\n</code></pre> <pre><code>def show_rate_limit(response: Response) -&gt; None:\n    h = response.headers\n    reset_time = datetime.fromtimestamp(int(h[\"X-RateLimit-Reset\"]))\n    now = datetime.now()\n    if reset_time &gt; now:\n        print(\n            f\"Requests remaining: {h['X-RateLimit-Remaining']}/{h['X-RateLimit-Limit']}\"\n            f\"\\nTime until reset: {str(reset_time - now).split('.')[0]}\",\n        )\n    else:\n        print(\"The rate limit has been reset.\")\n</code></pre>","tags":["REST API","Series: GitHub repo issues dataset"]},{"location":"get-github-repo-issues-comments/#caveats-and-workarounds","title":"Caveats and workarounds","text":"<p>Unfortunately, GitHub imposes a 300-page limit on paginated query results. To get more than 30000 issue comments, a workaround is to query for time-sorted entries and restrict the query range with the <code>since</code> parameter. However, because <code>since</code> is based on the last update time, we should set <code>sort</code> and <code>direction</code> accordingly to best avoid overlaps between queries.</p> <p>The wrapper interfaces and their usage are as follows:</p> <pre><code># https://docs.github.com/en/rest/issues/issues#list-repository-issues\ndef get_issues(\n    repo: str,\n    start_page: int = 1,\n    end_page: int | None = None,\n    *,\n    token: str | None = None,\n    per_page: int = 100,\n    state: Literal[\"all\", \"open\", \"closed\"] = \"all\",\n    sort: Literal[\"created\", \"updated\", \"comments\"] = \"created\",\n    direction: Literal[\"asc\", \"desc\"] = \"desc\",\n    since: str | None = None,  # \u27e8YYYY\u27e9-\u27e8MM\u27e9-\u27e8DD\u27e9T\u27e8hh\u27e9-\u27e8mm\u27e9-\u27e8ss\u27e9Z\n    **query: Any,\n) -&gt; tuple[list[dict[str, Any]], Response]:\n    return get_paginated(\n        f\"/repos/{repo}/issues\",\n        start_page=start_page,\n        end_page=end_page,\n        token=token,\n        per_page=per_page,\n        state=state,\n        sort=sort,\n        direction=direction,\n        since=since,\n        **query,\n    )\n\n\n# https://docs.github.com/en/rest/issues/comments#list-issue-comments-for-a-repository\ndef get_issue_comments(\n    repo: str,\n    start_page: int = 1,\n    end_page: int | None = None,\n    *,\n    token: str | None = None,\n    per_page: int = 100,\n    sort: Literal[\"created\", \"updated\"] = \"created\",\n    direction: Literal[\"asc\", \"desc\"] = \"desc\",\n    since: str | None = None,  # \u27e8YYYY\u27e9-\u27e8MM\u27e9-\u27e8DD\u27e9T\u27e8hh\u27e9-\u27e8mm\u27e9-\u27e8ss\u27e9Z\n    **query: Any,\n) -&gt; tuple[list[dict[str, Any]], Response]:\n    return get_paginated(\n        f\"/repos/{repo}/issues/comments\",\n        start_page=start_page,\n        end_page=end_page,\n        token=token,\n        per_page=per_page,\n        sort=sort,\n        direction=direction,\n        since=since,\n        **query,\n    )\n</code></pre> <pre><code>issues, response = get_issues(\n    REPO,\n    token=userdata.get(SECRET) if SECRET else None,\n    state=\"all\",\n    sort=\"updated\",\n    direction=\"asc\",\n)\n</code></pre> <pre><code>  0%|          | 0/125 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>show_rate_limit(response)\n</code></pre> <pre><code>Requests remaining: 4875/5000\nTime until reset: 0:58:30\n</code></pre> <pre><code>comments, response = get_issue_comments(\n    REPO,\n    token=userdata.get(SECRET) if SECRET else None,\n    sort=\"updated\",\n    direction=\"asc\",\n)\n</code></pre> <pre><code>  0%|          | 0/300 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>comments[-1][\"updated_at\"]\n</code></pre> <pre><code>'2025-02-06T23:52:33Z'\n</code></pre> <pre><code>comments_2, response = get_issue_comments(\n    REPO,\n    token=userdata.get(SECRET) if SECRET else None,\n    sort=\"updated\",\n    direction=\"asc\",\n    since=\"2025-02-06T23:52:34Z\",\n)\n</code></pre> <pre><code>  0%|          | 0/40 [00:00&lt;?, ?it/s]\n</code></pre> <pre><code>comments.extend(comments_2)\n</code></pre> <pre><code>show_rate_limit(response)\n</code></pre> <pre><code>Requests remaining: 4535/5000\nTime until reset: 0:49:19\n</code></pre>","tags":["REST API","Series: GitHub repo issues dataset"]},{"location":"get-github-repo-issues-comments/#saving-query-results-as-json-lines","title":"Saving query results as JSON Lines","text":"<pre><code>import json\nfrom collections.abc import Iterable\nfrom typing import Any\n\n\ndef load_jsonl(path: str) -&gt; list[Any]:\n    with open(path, \"r\") as file:\n        return [json.loads(line) for line in file]\n\n\ndef save_jsonl(objs: Iterable[Any], path: str, mode: str = \"a\") -&gt; None:\n    with open(path, mode=mode) as file:\n        for obj in objs:\n            file.write(json.dumps(obj) + \"\\n\")\n</code></pre> <pre><code>save_jsonl(issues, \"issues.jsonl\")\nsave_jsonl(comments, \"comments.jsonl\")\n</code></pre> <pre><code>assert issues == load_jsonl(\"issues.jsonl\")\nassert comments == load_jsonl(\"comments.jsonl\")\n</code></pre>","tags":["REST API","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/","title":"Making a Hugging Face dataset of GitHub repo issues","text":"<p>This is Part II of my adaptation of the tutorial  Hugging Face NLP Course: Creating your own dataset.</p> <p>Cleaned dataset on Hugging Face Hub:  dd-n-kk/uv-github-issues</p> <p>  Colab notebook </p>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#preparations","title":"Preparations","text":"<pre><code># Set this to an empty string to avoid saving the dataset.\nDATA_DIR = \"uv-github-issues/\"\n\n# Set these to empty strings to avoid uploading the dataset.\nREPO_ID = \"dd-n-kk/uv-github-issues\"\nSECRET = \"HF_TOKEN\"\n</code></pre> <pre><code>!uv pip install --system -Uq polars\n</code></pre> <pre><code>import polars as pl\nfrom polars import col\n</code></pre> <pre><code>SEED = 777\npl.set_random_seed(SEED)\n_ = pl.Config(\n    tbl_cols=-1,\n    tbl_rows=100,\n    tbl_width_chars=-1,\n    float_precision=3,\n    fmt_str_lengths=200,\n    fmt_table_cell_list_len=-1,\n)\n</code></pre> <p>The raw data were collected in Part I. To properly read a JSON Lines file into a Polars DataFrame, <code>read_ndjson()</code> may need an increased <code>infer_schema_length</code>.</p> <pre><code>issues_df = pl.read_ndjson(\"issues.jsonl\", infer_schema_length=1000)\ncomments_df = pl.read_ndjson(\"comments.jsonl\")\n</code></pre>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#verifying-completeness-of-data-collection","title":"Verifying completeness of data collection","text":"<p>First, we use a join to check whether we collected as many comments as recorded by GitHub for each issue. It turns out that there are indeed a handful of mismatches. However, according to the GitHub web pages (at the time of this post), the numbers of the collected comments are more correct.</p> <pre><code>(\n    issues_df.lazy()\n    .select(\"url\", \"html_url\", \"comments\")\n    .join(\n        comments_df.lazy().group_by(\"issue_url\").agg(pl.len().alias(\"collected_comments\")),\n        how=\"left\",\n        left_on=\"url\",\n        right_on=\"issue_url\",\n    )\n    .filter(col(\"comments\") != col(\"collected_comments\"))\n    .collect()\n)\n</code></pre> shape: (4, 4)urlhtml_urlcommentscollected_commentsstrstri64u32\"https://api.github.com/repos/astral-sh/uv/issues/6700\"\"https://github.com/astral-sh/uv/issues/6700\"98\"https://api.github.com/repos/astral-sh/uv/issues/8635\"\"https://github.com/astral-sh/uv/issues/8635\"98\"https://api.github.com/repos/astral-sh/uv/issues/8858\"\"https://github.com/astral-sh/uv/pull/8858\"21\"https://api.github.com/repos/astral-sh/uv/issues/10230\"\"https://github.com/astral-sh/uv/issues/10230\"43","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#simplifying-the-issues-dataframe","title":"Simplifying the issues DataFrame","text":"<ul> <li> <p>Discarded columns:</p> <ul> <li>Null columns:<ul> <li><code>active_lock_reason</code></li> <li><code>performed_via_github_app</code></li> <li><code>type</code></li> </ul> </li> <li>Redundant or easily recoverable columns:<ul> <li><code>assignee</code></li> <li><code>comments_url</code></li> <li><code>events_url</code></li> <li><code>labels_url</code></li> <li><code>repository_url</code></li> <li><code>timeline_url</code></li> </ul> </li> <li>Columns with little info:<ul> <li><code>sub_issues_summary</code></li> </ul> </li> </ul> </li> <li> <p>Simplified columns:</p> <ul> <li>Structs in <code>user</code>, <code>closed_by</code>, and <code>assignees</code> are replaced   with their <code>login</code> fields.</li> <li>Structs in <code>labels</code> are replaced with their <code>name</code> fields.</li> <li>Structs in <code>milestone</code> are replaced with their <code>number</code> fields.</li> <li>Values in <code>pull_request</code> are replaced with booleans   of whether they were previously non-nulls.   Also, the <code>merged_at</code> fields are extracted into a standalone column.</li> <li>The <code>+1</code>, <code>-1</code>, <code>laugh</code>, <code>hooray</code>, <code>confused</code>, <code>heart</code>, <code>rocket</code>, and <code>eyes</code> fields   of the Structs in <code>reactions</code> are extracted into standalone columns,   with <code>+1</code> and <code>-1</code> renamed to <code>upvote</code> and <code>downvote</code>, respectively.   The <code>reactions</code> column is then dropped.</li> </ul> </li> <li> <p>Since GitHub REST API uses a specific ISO 8601 format   <code>\u27e8YYYY\u27e9-\u27e8MM\u27e9-\u27e8DD\u27e9T\u27e8hh\u27e9-\u27e8mm\u27e9-\u27e8ss\u27e9Z</code>,   I decide to preserve the timestamp columns in this format (e.g. <code>updated_at</code>) as is.</p> </li> </ul> <pre><code>issues_df.select(pl.selectors.by_dtype(pl.Null)).columns\n</code></pre> <pre><code>['type', 'active_lock_reason', 'performed_via_github_app']\n</code></pre> <pre><code>issues_df = (\n    issues_df.lazy()\n    .select(\n        col(\"id\"),\n        col(\"node_id\"),\n        col(\"number\"),\n        col(\"url\"),\n        col(\"html_url\"),\n\n        col(\"title\"),\n        col(\"body\"),\n\n        col(\"user\").struct.field(\"login\").alias(\"user\"),\n        col(\"author_association\"),\n\n        col(\"labels\").list.eval(pl.element().struct.field(\"name\")),\n        col(\"pull_request\").is_not_null(),\n        col(\"draft\"),\n        col(\"milestone\").struct.field(\"number\").alias(\"milestone\"),\n\n        col(\"state\"),\n        col(\"state_reason\"),\n        col(\"locked\"),\n        col(\"assignees\").list.eval(pl.element().struct.field(\"login\")),\n        col(\"closed_by\").struct.field(\"login\").alias(\"closed_by\"),\n\n        col(\"created_at\"),\n        col(\"updated_at\"),\n        col(\"pull_request\").struct.field(\"merged_at\"),\n        col(\"closed_at\"),\n\n        col(\"reactions\").struct.field(\"+1\").alias(\"upvote\"),\n        col(\"reactions\").struct.field(\"-1\").alias(\"downvote\"),\n        col(\"reactions\").struct.field(\"laugh\"),\n        col(\"reactions\").struct.field(\"hooray\"),\n        col(\"reactions\").struct.field(\"confused\"),\n        col(\"reactions\").struct.field(\"heart\"),\n        col(\"reactions\").struct.field(\"rocket\"),\n        col(\"reactions\").struct.field(\"eyes\"),\n\n        col(\"comments\"),\n    )\n    .collect()\n)\n</code></pre> <p>Some example queries on the simplified issues DataFrame:</p> <pre><code># Top 5 most upvoted issue titles:\n(\n    issues_df.select(\"title\", \"upvote\")\n    .sort(\"upvote\", descending=True)\n    .head(5)\n)\n</code></pre> shape: (5, 2)titleupvotestri64\"Add option to upgrade all packages in the environment, e.g., `upgrade --all`\"258\"Using `uv run` as a task runner\"257\"Allow creating a `python` shim on `python install`\"216\"Add a command to activate the virtual environment, e.g., `uv shell`\"213\"Add a command to read and update (i.e., bump) the project version, e.g., `uv version`\"177 <pre><code># Top 5 pull request authors:\n(\n    issues_df.filter(col(\"pull_request\"))\n    .get_column(\"user\").value_counts()\n    .sort(\"count\", descending=True)\n    .head(5)\n)\n</code></pre> shape: (5, 2)usercountstru32\"charliermarsh\"2728\"zanieb\"1342\"konstin\"804\"renovate[bot]\"472\"BurntSushi\"139","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#simplifying-the-comments-dataframe","title":"Simplifying the comments DataFrame","text":"<ul> <li>The null column <code>performed_via_github_app</code> is discarded.</li> <li><code>user</code> and <code>reactions</code> are simplified the same way as the issues DataFrame.</li> <li><code>issue_url</code> corresponds to <code>url</code> in the issues DataFrame,   so it can serve as a join column. But for convenience I additionally create   an <code>issue_number</code> column, which corresponds to <code>number</code> in the issues DataFrame.</li> </ul> <pre><code>comments_df = (\n    comments_df.lazy()\n    .select(\n        col(\"id\"),\n        col(\"node_id\"),\n        col(\"url\"),\n        col(\"html_url\"),\n\n        col(\"issue_url\").str.extract(r\"/(\\d+)$\").cast(pl.Int64).alias(\"issue_number\"),\n        col(\"issue_url\"),\n\n        col(\"body\"),\n\n        col(\"user\").struct.field(\"login\").alias(\"user\"),\n        col(\"author_association\"),\n\n        col(\"created_at\"),\n        col(\"updated_at\"),\n\n        col(\"reactions\").struct.field(\"+1\").alias(\"upvotes\"),\n        col(\"reactions\").struct.field(\"-1\").alias(\"downvotes\"),\n        col(\"reactions\").struct.field(\"laugh\"),\n        col(\"reactions\").struct.field(\"hooray\"),\n        col(\"reactions\").struct.field(\"confused\"),\n        col(\"reactions\").struct.field(\"heart\"),\n        col(\"reactions\").struct.field(\"rocket\"),\n        col(\"reactions\").struct.field(\"eyes\"),\n    )\n    .collect()\n)\n</code></pre>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#organizing-and-sharing-the-hugging-face-dataset","title":"Organizing and sharing the Hugging Face dataset","text":"<p>I decide to organize the issues and comments DataFrames as separate sub-datasets so that the users can choose to attach the comments data to each issue via joining.</p>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#creating-the-train-test-splits","title":"Creating the train-test splits","text":"<p>Semi-join is used to make sure the train-test splits of the comments subset match correctly with the issues subset.</p> <pre><code>TEST_SIZE = int(len(issues_df) * 0.2)\n\nissues_df = issues_df.sample(len(issues_df), shuffle=True, seed=SEED)\nissues_df_train = issues_df.head(len(issues_df) - TEST_SIZE)\nissues_df_test = issues_df.tail(TEST_SIZE)\n</code></pre> <pre><code>comments_df_train = comments_df.join(\n    issues_df_train, how=\"semi\", left_on=\"issue_number\", right_on=\"number\"\n)\ncomments_df_test = comments_df.join(\n    issues_df_test, how=\"semi\", left_on=\"issue_number\", right_on=\"number\"\n)\n</code></pre> <pre><code>if DATA_DIR:\n    import os\n\n    os.makedirs(DATA_DIR, exist_ok=True)\n    issues_df_train.write_ndjson(f\"{DATA_DIR}/issues-train.jsonl\")\n    issues_df_test.write_ndjson(f\"{DATA_DIR}/issues-test.jsonl\")\n    comments_df_train.write_ndjson(f\"{DATA_DIR}/comments-train.jsonl\")\n    comments_df_test.write_ndjson(f\"{DATA_DIR}/comments-test.jsonl\")\n</code></pre>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#configuring-the-subsets","title":"Configuring the subsets","text":"<pre><code>config_str = \"\"\"---\nconfigs:\n- config_name: issues\n  data_files:\n  - split: train\n    path: \"issues-train.jsonl\"\n  - split: test\n    path: \"issues-test.jsonl\"\n  default: true\n- config_name: comments\n  data_files:\n  - split: train\n    path: \"comments-train.jsonl\"\n  - split: test\n    path: \"comments-test.jsonl\"\n---\n\"\"\"\n\nif DATA_DIR:\n    with open(f\"{DATA_DIR}/README.md\", \"w\") as file:\n        file.write(config_str)\n</code></pre>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"make-huggingface-dataset-of-github-repo-issues/#uploading-and-downloading-the-dataset","title":"Uploading and downloading the dataset","text":"<pre><code>if REPO_ID and SECRET:\n    from google.colab import userdata\n    !huggingface-cli upload uv-github-issues uv-github-issues/ . --repo-type dataset --token={userdata.get(SECRET)}\n</code></pre> <pre><code>Consider using `hf_transfer` for faster uploads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\nStart hashing 5 files.\nFinished hashing 5 files.\ncomments-train.jsonl:   0% 0.00/28.5M [00:00&lt;?, ?B/s]\nUpload 2 LFS files:   0% 0/2 [00:00&lt;?, ?it/s]\u001b[A\n\ncomments-train.jsonl:  13% 3.60M/28.5M [00:00&lt;00:00, 35.8MB/s]\n\nissues-train.jsonl:   2% 344k/18.8M [00:00&lt;00:05, 3.32MB/s]\u001b[A\u001b[A\n\ncomments-train.jsonl:  25% 7.19M/28.5M [00:00&lt;00:02, 7.14MB/s]\n\ncomments-train.jsonl:  31% 8.95M/28.5M [00:01&lt;00:02, 7.98MB/s]\n\ncomments-train.jsonl:  39% 11.0M/28.5M [00:01&lt;00:01, 8.91MB/s]\n\ncomments-train.jsonl:  51% 14.5M/28.5M [00:01&lt;00:01, 12.9MB/s]\n\ncomments-train.jsonl:  78% 22.1M/28.5M [00:02&lt;00:00, 13.4MB/s]\n\ncomments-train.jsonl: 100% 28.5M/28.5M [00:03&lt;00:00, 7.67MB/s]\nissues-train.jsonl: 100% 18.8M/18.8M [00:03&lt;00:00, 5.02MB/s]\n\nUpload 2 LFS files:  50% 1/2 [00:04&lt;00:04,  4.15s/it]\u001b[A\nUpload 2 LFS files: 100% 2/2 [00:05&lt;00:00,  2.64s/it]\nhttps://huggingface.co/datasets/dd-n-kk/uv-github-issues/tree/main/.\n</code></pre> <pre><code>!uv pip install --system -q datasets\n</code></pre> <pre><code>from datasets import load_dataset\n</code></pre> <pre><code>issues_ds = load_dataset(\"dd-n-kk/uv-github-issues\")\n</code></pre> <pre><code>README.md:   0%|          | 0.00/299 [00:00&lt;?, ?B/s]\n\n\n\nissues-train.jsonl:   0%|          | 0.00/18.8M [00:00&lt;?, ?B/s]\n\n\n\nissues-test.jsonl:   0%|          | 0.00/4.89M [00:00&lt;?, ?B/s]\n\n\n\nGenerating train split:   0%|          | 0/9963 [00:00&lt;?, ? examples/s]\n\n\n\nGenerating test split:   0%|          | 0/2490 [00:00&lt;?, ? examples/s]\n</code></pre> <pre><code>comments_ds = load_dataset(\"dd-n-kk/uv-github-issues\", \"comments\")\n</code></pre> <pre><code>comments-train.jsonl:   0%|          | 0.00/28.5M [00:00&lt;?, ?B/s]\n\n\n\ncomments-test.jsonl:   0%|          | 0.00/7.18M [00:00&lt;?, ?B/s]\n\n\n\nGenerating train split:   0%|          | 0/27293 [00:00&lt;?, ? examples/s]\n\n\n\nGenerating test split:   0%|          | 0/6682 [00:00&lt;?, ? examples/s]\n</code></pre>","tags":["Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"polars-stratified-train-test-split/","title":"Stratified train-test split with Polars","text":"<p>  Colab notebook </p>","tags":["Polars"]},{"location":"polars-stratified-train-test-split/#preparations","title":"Preparations","text":"<pre><code>!uv pip install -Uq polars\n</code></pre> <pre><code>import numpy as np\nimport polars as pl\n</code></pre> <pre><code>_ = pl.Config(\n    float_precision=3,\n    fmt_str_lengths=200,\n    fmt_table_cell_list_len=-1,\n    tbl_cols=-1,\n    tbl_rows=100,\n    tbl_width_chars=100,\n)\n\nrng = np.random.default_rng(seed=777)\n</code></pre>","tags":["Polars"]},{"location":"polars-stratified-train-test-split/#dummy-data-set","title":"Dummy data set","text":"<pre><code>labels = rng.choice(4, size=1000, p=[0.1, 0.2, 0.3, 0.4])\nfeatures = rng.standard_normal((1000, 2)) * 0.1 + labels[:, None]\n\ndata = (\n    pl.concat(\n        (\n            pl.from_numpy(features, schema=[\"feat_1\", \"feat_2\"]),\n            pl.from_numpy(labels, schema=[\"label\"]),\n        ),\n        how=\"horizontal\",\n    )\n    .with_row_index(name=\"id\")\n)\n</code></pre> <pre><code>data.sample(5)\n</code></pre> shape: (5, 4)idfeat_1feat_2labelu32f64f64i646971.1210.94611742.9383.04739412.8813.01038671.9952.09827652.8203.1013 <pre><code>data.get_column(\"label\").value_counts().sort(\"label\")\n</code></pre> shape: (4, 2)labelcounti64u32098121122973394","tags":["Polars"]},{"location":"polars-stratified-train-test-split/#stratified-train-test-split","title":"Stratified train-test split","text":"","tags":["Polars"]},{"location":"polars-stratified-train-test-split/#train-split","title":"Train split","text":"<pre><code>train_split = data.select(\n    pl.all()\n    .sample(fraction=0.9, shuffle=True, seed=777)\n    .over(\"label\", mapping_strategy=\"explode\")\n)\n</code></pre> <pre><code>train_split.sample(5)\n</code></pre> shape: (5, 4)idfeat_1feat_2labelu32f64f64i648602.9462.85238761.8931.87523273.0353.0233657-0.1700.1060607-0.1880.2410 <pre><code>train_split.shape\n</code></pre> <pre><code>(898, 4)\n</code></pre> <pre><code>train_split.get_column(\"label\").value_counts(normalize=True).sort(\"label\")\n</code></pre> shape: (4, 2)labelproportioni64f6400.09810.21020.29730.394","tags":["Polars"]},{"location":"polars-stratified-train-test-split/#test-or-validation-split","title":"Test (or validation) split","text":"<pre><code>test_split = data.join(train_split, on=\"id\", how=\"anti\")\n</code></pre> <pre><code>test_split.shape\n</code></pre> <pre><code>(102, 4)\n</code></pre> <pre><code>test_split.get_column(\"label\").value_counts(normalize=True).sort(\"label\")\n</code></pre> shape: (4, 2)labelproportioni64f6400.09810.21620.29430.392","tags":["Polars"]},{"location":"retrieve-info-from-github-repo-issues/","title":"Retrieving information from a GitHub repo issues dataset","text":"<p>This is Part III of my adaptation of  Hugging Face NLP Course: Creating your own dataset. It consists of several parts:</p> <ol> <li>Creating a corpus of issue-comment pairs from the previously prepared dataset.</li> <li>Embedding each issue-comment pair into dense vectors for similarity search.</li> <li>Building a Faiss index of the embeddings to speed up querying.</li> <li>Using a reranker to pick the best entries from the top-k similarity search results.</li> </ol> <p>  Colab notebook </p>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#preparations","title":"Preparations","text":"<pre><code>!uv pip install -Uq polars\n!uv pip install -q datasets faiss-cpu\n</code></pre> <pre><code>from collections.abc import Sequence\n\nimport faiss\nimport numpy as np\nimport polars as pl\nimport torch as tc\nfrom datasets import load_dataset\nfrom numpy.typing import NDArray\nfrom polars import col\nfrom torch.nn import functional as F\nfrom tqdm.auto import trange\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n</code></pre> <pre><code>_ = pl.Config(\n    float_precision=3,\n    fmt_str_lengths=200,\n    fmt_table_cell_list_len=-1,\n    tbl_cols=-1,\n    tbl_rows=100,\n    tbl_width_chars=-1,\n)\n</code></pre>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#preparing-the-corpus","title":"Preparing the corpus","text":"<p>We directly download the <code>dd-n-kk/uv-github-issues</code> dataset prepared in Part II.</p> <pre><code>%%capture\nissues = load_dataset(\"dd-n-kk/uv-github-issues\", \"issues\")\ncomments = load_dataset(\"dd-n-kk/uv-github-issues\", \"comments\")\n</code></pre> <p>The train-test splits are merged because all processed entries will be used for querying.</p> <pre><code>issues_df = pl.concat([issues[\"train\"].to_polars(), issues[\"test\"].to_polars()])\ncomments_df = pl.concat([comments[\"train\"].to_polars(), comments[\"test\"].to_polars()])\n</code></pre>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#issues","title":"Issues","text":"<p>I decide to remove:</p> <ul> <li>Issues with null bodies.</li> <li>Issues created by bots, because they usually contain little info.</li> <li>Pull requests not yet merged, for they often contain suggestions not yet adopted.</li> </ul> <pre><code>issues_df = issues_df.filter(\n    col(\"body\").is_not_null()\n    &amp; (~col(\"user\").str.contains(\"[bot]\", literal=True))\n    &amp; (~col(\"pull_request\") | col(\"merged_at\").is_not_null())\n)\n</code></pre> <p>A crude word count reveals a problem: A small number of issues are extremely long.</p> <pre><code>q = (\n    issues_df.select(\n        \"html_url\", \"title\", col(\"body\").str.split(\" \").list.len().alias(\"n_words\")\n    )\n    .sort(\"n_words\", descending=True)\n)\nq.get_column(\"n_words\").describe()\n</code></pre> shape: (9, 2)statisticvaluestrf64\"count\"10313.000\"null_count\"0.000\"mean\"152.041\"std\"424.822\"min\"1.000\"25%\"21.000\"50%\"62.000\"75%\"151.000\"max\"12590.000 <p>From the GitHub web pages we can see that they contain long debug outputs, which are unlikely to help answer user questions.</p> <pre><code>q.head(5)\n</code></pre> shape: (5, 3)html_urltitlen_wordsstrstru32\"https://github.com/astral-sh/uv/issues/6443\"\"`uv sync` freezes infinitely at the container root\"12590\"https://github.com/astral-sh/uv/issues/5742\"\"Allow `uv sync --no-build-isolation`\"11862\"https://github.com/astral-sh/uv/issues/5046\"\"Bad resolver error for `colabfold[alphafold]==1.5.5` on python 3.11\"11764\"https://github.com/astral-sh/uv/issues/7183\"\"Improve Python version resolution UI\"11316\"https://github.com/astral-sh/uv/issues/2062\"\"uv pip and python -m pip resolve different versions of tensorflow in pyhf developer environment\"8819 <p>To shorten these issues, I:</p> <ul> <li>Replace Markdown fenced code blocks containing too many characters with <code>[CODE]</code>.</li> <li>Replace each HTML element <code>&lt;detail&gt;</code> with <code>[DETAIL]</code>.</li> <li>Replace HTML comments <code>&lt;!-- ... --&gt;</code> with <code>[COMMENT]</code>.</li> <li>Remove trailing whitespaces.</li> </ul> <pre><code>issues_df = (\n    issues_df.lazy()\n    .select(\n        \"number\",\n        col(\"html_url\").alias(\"issue_url\"),\n        \"title\",\n        (\n            col(\"body\").str.replace_all(r\"\\s*[\\r\\n]\", \"\\n\")\n            .str.replace_all(r\"(?s)&lt;details&gt;.*?&lt;/details&gt;\", \"[DETAILS]\")\n            .str.replace_all(r\"(?s)&lt;!--.*?--&gt;\", \"[COMMENT]\")\n            .str.replace_all(r\"```(?:[^`]|`[^`]|``[^`]){768,}```\", \"[CODE]\")\n            .str.replace_all(r\"~~~(?:[^~]|~[^~]|~~[^~]){768,}~~~\", \"[CODE]\")\n        ),\n    )\n    .collect()\n)\n</code></pre> <p>An issue body containing long debug outputs now looks like this:</p> <pre><code>print(issues_df.filter(col(\"number\") == 6443).item(0, \"body\"))\n</code></pre> <pre><code>To reproduce, try building the following Dockerfile (remove `sudo` if your docker is rootless):\n```\ncat &lt;&lt;EOF | sudo BUILDKIT_PROGRESS=plain docker build -\nFROM library/python:3.11\nRUN pip install 'uv == 0.3.1' \\\n    &amp;&amp; printf &gt;pyproject.toml '\\\n      [project]\\n\\\n      dependencies = [\"django ~= 4.2\"]\\n\\\n      name = \"demo\"\\n\\\n      version = \"0.1.0\"\\n\\\n      requires-python = \"&gt;=3.11.7\"\\n\\\n    '\\\n    &amp;&amp; uv lock\nRUN uv sync -vv\nEOF\n```\nThis is not specific to Django, according to my experiments.\nThe build freezes after `uv_build::run_python_script script=\"get_requires_for_build_editable\", python_version=3.11.9` verbose log, keeping a high CPU load for a few minutes.\n[DETAILS]\nHowever, this only happens if I build this at the root of filesystem. Adding `WORKDIR /home` before installation recovers everything, the build completes in seconds.\n[DETAILS]\n</code></pre> <p>The word counts are now subtantially reduced.</p> <pre><code>issues_df.get_column(\"body\").str.split(\" \").list.len().describe()\n</code></pre> shape: (9, 2)statisticvaluestrf64\"count\"10313.000\"null_count\"0.000\"mean\"81.815\"std\"105.018\"min\"1.000\"25%\"19.000\"50%\"51.000\"75%\"110.000\"max\"3260.000","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#comments","title":"Comments","text":"<p>The comments dataset is processed similarly: Bot comments are removed and long code blocks are snipped.</p> <pre><code>comments_df = comments_df.filter(~col(\"user\").str.contains(\"[bot]\", literal=True))\n</code></pre> <pre><code>comments_df.get_column(\"body\").str.split(\" \").list.len().describe()\n</code></pre> shape: (9, 2)statisticvaluestrf64\"count\"33789.000\"null_count\"0.000\"mean\"69.231\"std\"454.410\"min\"1.000\"25%\"12.000\"50%\"25.000\"75%\"55.000\"max\"39896.000 <pre><code>comments_df = (\n    comments_df.lazy()\n    .select(\n        \"issue_number\",\n        col(\"html_url\").alias(\"comment_url\"),\n        (\n            col(\"body\").str.replace_all(r\"\\s*[\\r\\n]\", \"\\n\")\n            .str.replace_all(r\"(?s)&lt;details&gt;.*?&lt;/details&gt;\", \"[DETAILS]\")\n            .str.replace_all(r\"(?s)&lt;!--.*?--&gt;\", \"[COMMENT]\")\n            .str.replace_all(r\"```(?:[^`]|`[^`]|``[^`]){768,}```\", \"[CODE]\")\n            .str.replace_all(r\"~~~(?:[^~]|~[^~]|~~[^~]){768,}~~~\", \"[CODE]\")\n            .alias(\"comment_body\")\n        ),\n    )\n    .collect()\n)\n</code></pre> <pre><code>comments_df.get_column(\"comment_body\").str.split(\" \").list.len().describe()\n</code></pre> shape: (9, 2)statisticvaluestrf64\"count\"33789.000\"null_count\"0.000\"mean\"43.568\"std\"62.287\"min\"1.000\"25%\"12.000\"50%\"24.000\"75%\"51.000\"max\"1934.000","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#joining","title":"Joining","text":"<p>We are ready to create the corpus. I've considered combining each issue with all associated comments, but that may require an embedding model with a very large context length. Therefore, I use left join to create issue-comment pairs while preserving issues with no comment.</p> <p>URLs are also collected for convenient lookups.</p> <pre><code>corpus_df = (\n    issues_df.lazy()\n    .join(comments_df.lazy(), how=\"left\", left_on=\"number\", right_on=\"issue_number\")\n    .select(\n        (\n            pl.when(col(\"comment_url\").is_null())\n            .then(col(\"issue_url\"))\n            .otherwise(\"comment_url\")\n            .alias(\"url\")\n        ),\n        (\n            pl.when(col(\"comment_body\").is_null())\n            .then(\n                pl.format(\"Issue {}: {}\\n\\n{}\", col(\"number\"), col(\"title\"), col(\"body\"))\n            )\n            .otherwise(\n                pl.format(\n                    \"Issue {}: {}\\n\\n{}\\n\\nComment:\\n{}\",\n                    col(\"number\"),\n                    col(\"title\"),\n                    col(\"body\"),\n                    col(\"comment_body\"),\n                )\n            )\n            .alias(\"text\")\n        ),\n    )\n    .sort(\"url\")\n    .collect()\n)\n</code></pre> <pre><code>assert corpus_df.get_column(\"url\").n_unique() == len(corpus_df)\n</code></pre> <pre><code>corpus_df.get_column(\"text\").str.split(\" \").list.len().describe()\n</code></pre> shape: (9, 2)statisticvaluestrf64\"count\"35278.000\"null_count\"0.000\"mean\"162.174\"std\"149.027\"min\"3.000\"25%\"68.000\"50%\"128.000\"75%\"213.000\"max\"3312.000","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#embedding-issue-comment-pairs","title":"Embedding issue-comment pairs","text":"<p>\u200b</p> <p>Running this section likely requires GPU.</p> <p>I pick <code>BAAI/bge-m3</code> as the pretrained embedder. It is based on <code>FacebookAI/xlm-roberta-large</code>. It is reasonably sized for a Colab T4 GPU, has a long enough context length of 8192, and is versatile and efficient.</p> <pre><code>corpus = corpus_df.get_column(\"text\").to_list()\n</code></pre> <pre><code>%%capture\nEMB_CKPT = \"BAAI/bge-m3\"\ntokenizer = AutoTokenizer.from_pretrained(EMB_CKPT)\nembedder = AutoModel.from_pretrained(EMB_CKPT)\n</code></pre> <p>To reduce unncessary padding, the padding length is determined batch by batch. Also, the encoded entries are batched in decreasing lengths, so that the batch maximum length accomodates the entries efficiently. We do have to restore the embeddings to the original order.</p> <pre><code>def embed(\n    texts: Sequence[str],\n    *,\n    tokenizer,\n    embedder,\n    batch_size: int,\n    context_len: int,\n    device=None,\n    use_half: bool = True,\n) -&gt; NDArray:\n    no_tqdm = len(texts) &lt; batch_size\n    if device is None:\n        device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n\n    encodings = []\n    for i in trange(0, len(texts), batch_size, desc=\"Tokenization\", disable=no_tqdm):\n        # No padding now; pad within each embedder input batch.\n        batch = tokenizer(\n            texts[i : i + batch_size], truncation=True, max_length=context_len\n        )\n\n        # dict[list] -&gt; list[dict]\n        ## Just one way to conform to `tokenizer.pad()`.\n        encodings.extend(dict(zip(batch, vals)) for vals in zip(*batch.values()))\n\n    # Sort by token count in descending order to reduce padding.\n    # Keep the sorted index to restore the original order later.\n    ## Reverse view &gt; element-wise negative (https://stackoverflow.com/a/16486305)\n    sorted_index = np.argsort([len(x[\"input_ids\"]) for x in encodings])[::-1]\n    encodings = [encodings[i] for i in sorted_index]\n\n    embedder = embedder.to(device).eval()\n    # Using float16 only on GPU.\n    if device.type == \"cuda\" and use_half:\n        embedder = embedder.half()\n\n    embeddings = []\n    with tc.inference_mode():\n        for i in trange(\n            0,\n            len(encodings),\n            batch_size,\n            desc=\"Embedding\",\n            disable=len(encodings) &lt; batch_size,\n        ):\n            # Within-batch padding\n            ## `BatchEncoding` has method `to()`.\n            padded = tokenizer.pad(\n                encodings[i : i + batch_size],\n                padding=True,\n                return_tensors=\"pt\",\n            ).to(device)\n\n            # [CLS] pooling with normalization\n            embeddings.append(\n                F.normalize(embedder(**padded).last_hidden_state[:, 0], dim=-1)\n                .cpu()\n                .numpy()\n            )\n\n    # Merge, cast to float32 (for Faiss), and restore original order.\n    return np.concatenate(embeddings, 0, dtype=np.float32)[np.argsort(sorted_index)]\n</code></pre> <p>The embedding process takes about 10 minutes in a Colab T4 GPU runtime.</p> <pre><code>embeddings = embed(\n    corpus,\n    tokenizer=tokenizer,\n    embedder=embedder,\n    batch_size=32,\n    context_len=4096,\n    use_half=True,\n)\n</code></pre> <pre><code>Tokenization:   0%|          | 0/1103 [00:00&lt;?, ?it/s]\n\n\n\nEmbedding:   0%|          | 0/1103 [00:00&lt;?, ?it/s]\n\n\nYou're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n</code></pre> <pre><code>embeddings.shape\n</code></pre> <pre><code>(35278, 1024)\n</code></pre> <p>We can now create some example user questions about <code>astral-sh/uv</code> as queries.</p> <pre><code>queries = [\n    \"What is the difference between `uv pip install` and `uv add`?\",\n    \"How to update Python in my venv to the latest version?\",\n    \"How to install the CPU version of PyTorch?\",\n    \"Can I add a package dependency without version requirement?\",\n    \"What does the `.python-version` file do?\",\n]\n</code></pre> <pre><code>q_embeddings = embed(\n    queries,\n    tokenizer=tokenizer,\n    embedder=embedder,\n    batch_size=8,\n    context_len=512,\n    use_half=True,\n)\n</code></pre> <pre><code>%timeit (q_embeddings @ embeddings.T)\n</code></pre> <pre><code>55.6 ms \u00b1 1.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <p>The higher the inner product of a query embedding and an issue-comment pair embedding, the more similar they should be, and the more likely the issue-comment pair contains an answer to the question.</p> <pre><code>result_indexes = (q_embeddings @ embeddings.T).argmax(-1).tolist()\n</code></pre> <pre><code>def display_query_results(queries, corpus_df, indexes):\n    for query, i in zip(queries, indexes):\n        print(f\"Query: {query}\\n\")\n        print(f\"Result URL: {corpus_df.get_column('url')[i]}\\n\")\n        print(f\"Result:\\n{corpus_df.get_column('text')[i]}\\n\")\n        print(f\"==================================================\\n\")\n</code></pre> <p>Even though only some of the comments directly answer the questions, all the retrieved issues are indeed highly relevant.</p> <pre><code>display_query_results(queries, corpus_df, result_indexes)\n</code></pre> <pre><code>Query: What is the difference between `uv pip install` and `uv add`?\n\nResult URL: https://github.com/astral-sh/uv/issues/9219#issuecomment-2613016513\n\nResult:\nIssue 9219: What's the difference between `uv pip install` and `uv add`\n\n[COMMENT]\nI've been using `uv` for a while and I really enjoy it. I keep stumbling onto the same confusion though. I never quite know whether do to:\n```sh\nuv init\nuv venv\nuv add polars marimo\nuv run hello.py\n```\nor\n```sh\nuv init\nuv venv\nsource .venv/bin/activate\npip install polars marimo\npython hello.py\n```\nare these two above equivalent?\n---\nalso are these two equivalent?\n```\nuv add polars\n```\n```\nuv pip install polars\n```\n\nComment:\n1. `pip install` will only install into its own environment, `uv pip install` can target other environments.\n2. `uv run script.py` will activate a virtual environment if necessary, and read PEP 723 inline metadata or sync your project if necessary, then call `python script.py` \u2014 the latter just uses your current environment as-is.\n\n==================================================\n\nQuery: How to update Python in my venv to the latest version?\n\nResult URL: https://github.com/astral-sh/uv/issues/11317#issuecomment-2643263625\n\nResult:\nIssue 11317: How to update `uv` managed Python version for applications?\n\n### Question\nI have Python installed via uv's `uv python install 3.x`. The project is an project/application (so not a library).\nLet's say there is a new Python (standalone) release, e.g.\n3.12.9 -&gt; 3.13.2\n3.13.1 -&gt; 3.13.2\nIt there a way to 'update' the standalone Python from uv, also replacing the existing `.venv`?\nMy current solution is to install the new python with `uv python install 3.x`, deleting the .venv and recreating it with the target python version (and delete the `.python-version` file). This works fine, but is a rather tedious process.\nI would love to have a project scoped command like `uv sync --update-python-to 3.13.2`  (handling also the installation of the standalone-python).\nSimilar possibilities:\n- `uv python update` (either updating existing standalone python versions on patch level AND/OR updating project .venv on python version patch level)\n- `uv python update 3.13` (updating project .venv to latest specified python version)\nI guess there are smarter persons, having better ideas how to handle these commands. Is there something in uv right now, that I'm misssing?\n### Platform\n_No response_\n### Version\n0.5.29\n\nComment:\nDang, didn't found that one. Thx!\n\n==================================================\n\nQuery: How to install the CPU version of PyTorch?\n\nResult URL: https://github.com/astral-sh/uv/issues/11079#issuecomment-2624678978\n\nResult:\nIssue 11079: Add pytorch documentation section on how to install for Intel GPUs\n\n### Summary\nPytorch 2.6 [adds support for Intel GPUs](https://pytorch.org/docs/main/notes/get_start_xpu.html). The current [uv pytorch install docs](https://docs.astral.sh/uv/guides/integration/pytorch/#using-a-pytorch-index) include instructions for:\n- CPU only\n- Various CUDA versions\n- ROCm\nBut not yet for Intel GPUs.\nMy current attempt at mimicking the docs for existing GPUs is\n```pyproject.toml\n# pyproject.toml\n[project]\nname = \"pytorch-intel\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\"torch&gt;=2.6.0\"]\n[[tool.uv.index]]\nname = \"pytorch-intel-gpu\"\nurl = \"https://download.pytorch.org/whl/xpu\"\nexplicit = true\n[tool.uv.sources]\ntorch = [{ index = \"pytorch-intel-gpu\", marker = \"platform_system == 'Linux'\" }]\n```\n```python\n# hello.py\nimport torch\nprint(torch.xpu.is_available())\n```\nRunning `uv run hello.py` produces\n```sh\n  \u00d7 No solution found when resolving dependencies for split (sys_platform\n  \u2502 == 'linux'):\n  \u2570\u2500\u25b6 Because there is no version of pytorch-triton-xpu{platform_machine\n      == 'x86_64' and sys_platform == 'linux'}==3.2.0 and torch==2.6.0+xpu\n      depends on pytorch-triton-xpu{platform_machine == 'x86_64'\n      and sys_platform == 'linux'}==3.2.0, we can conclude that\n      torch==2.6.0+xpu cannot be used.\n      And because only the following versions of torch{sys_platform ==\n      'linux'} are available:\n          torch{sys_platform == 'linux'}&lt;2.6.0\n          torch{sys_platform == 'linux'}==2.6.0+xpu\n      and your project depends on torch{sys_platform == 'linux'}&gt;=2.6.0, we\n      can conclude that your project's requirements are unsatisfiable.\n```\n### Example\n_No response_\n\nComment:\nYes, this worked for me!\nI got a warning from pytorch that it couldn't initialize numpy. Adding numpy to the pyproject fixed the warning.\n\n==================================================\n\nQuery: Can I add a package dependency without version requirement?\n\nResult URL: https://github.com/astral-sh/uv/issues/6476#issuecomment-2305937941\n\nResult:\nIssue 6476: Allow adding a dependency with no version constraint\n\n`uv add` adds a lower bound version constraint by default. For example, calling `uv add requests` currently adds the dependency `\"requests&gt;=2.32.3\"` to `pyproject.toml`.\nIt's possible to have uv use different upper and/or lower bounds, but I could not find a way to add a dependency without any version constraint at all. For example, I want the dependency `\"requests\"` added to `pyproject.toml` without any bounds.\nThe current default behavior is fine, but I think there should be some way to add an unconstrained dependency, either with a global configuration setting that changes the default behavior, or with a command-line option for `uv add`.\nApologies if I missed something in the documentation and this is already possible. (Of course I could edit `pyproject.toml` manually, but it doesn't seem like that should be necessary.)\nThanks for the great work on a fantastic project!\n\nComment:\nThat seems ok to me.\n\n==================================================\n\nQuery: What does the `.python-version` file do?\n\nResult URL: https://github.com/astral-sh/uv/issues/8920#issuecomment-2465005344\n\nResult:\nIssue 8920: Purpose of .python-version?\n\n[COMMENT]\nCurrently on `uv init` a project boilerplate created includes the file `.python-version`.\nWhat is the purpose of this file, if there are constraints in `pyproject.toml` and `uv.lock`?\nIs it just for compatibility with tools like pyenv or is there more to it?\nI really like how clean project directory became after moving a lot of stuff (like `requirements.*`) to `pyproject.toml`. So wonder, if `.python-version` is really required in two senses:\n1. Is it required for the projects now? I tested by deleting `.python-version`, and everything works as expected and this file is not recreated with commands like `uv run`, `uv sync` and `uv lock`\n2. Since a lot of constraints/pinned versions of things are now in `pyproject.toml` and `uv.lock`, maybe if `.python-version` still plays some important role, its function can be moved to `pyproject.toml` or `uv.lock` in future?\nAsking as a 5-year user of `pyenv` and `.python-version`. They are great, but I don't really miss them.\nAlso, couldn't find peps related to this file.\n\nComment:\nRelated - https://github.com/astral-sh/uv/issues/8247\nA `.python-version` file is not strictly required, but it's useful to have it when developing a project as it allows you to specify the exact Python version you are using to do development. It's different to the `requires-python` field  - that is the range of Python versions supported by your project.\n\n\n==================================================\n</code></pre>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#building-a-faiss-index-to-improve-query-speed","title":"Building a Faiss index to improve query speed","text":"<p>Faiss is a library that can create indexes to speed up similarity search among dense vector embeddings, often at very little cost of accuracy.</p> <p>I use an inverted file index with inner product as metric. The <code>nlist</code> is the number of partitions made (in the form of inverted lists) in the embedding space, and the <code>nprob</code> is the number of partitions examined per query. They are set roughly according to the guideilnes.</p> <pre><code>D = embeddings.shape[-1]\nnlist = 2048\nnprob = 16\n</code></pre> <pre><code>quantizer = faiss.IndexFlatIP(D)\nfaiss_index = faiss.IndexIVFFlat(quantizer, D, nlist, faiss.METRIC_INNER_PRODUCT)\n</code></pre> <p>Building the index takes less than 1 minute.</p> <pre><code>faiss_index.train(embeddings)\nfaiss_index.add(embeddings)\n</code></pre> <pre><code>faiss_index.is_trained, faiss_index.ntotal, faiss_index.nprobe\n</code></pre> <pre><code>(True, 35278, 1)\n</code></pre> <pre><code>faiss_index.nprobe = nprob\n</code></pre> <p>The index improves query time by a factor of \\(\\sim 20\\), and happens to get exactly the same results from our example queries.</p> <pre><code>%timeit faiss_index.search(q_embeddings, k=1)\n</code></pre> <pre><code>2.71 ms \u00b1 69 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> <pre><code>metrics, faiss_result_indexes = faiss_index.search(q_embeddings, k=1)\nresult_indexes, faiss_result_indexes.reshape(-1).tolist()\n</code></pre> <pre><code>([25352, 2833, 2230, 18026, 24623], [25352, 2833, 2230, 18026, 24623])\n</code></pre>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"retrieve-info-from-github-repo-issues/#using-a-reranker-to-improve-query-results","title":"Using a Reranker to improve query results","text":"<p>A reranker is supposed to evaluate relevance between embeddings more accurately but slowly than distance metrics.</p> <p>Here I use <code>BAAI/bge-reranker-v2-m3</code>, the matching reranker of <code>BAAI/bge-m3</code>.</p> <pre><code>%%capture\nRRK_CKPT = \"BAAI/bge-reranker-v2-m3\"\nrerank_tokenizer = AutoTokenizer.from_pretrained(RRK_CKPT)\nreranker = AutoModelForSequenceClassification.from_pretrained(RRK_CKPT)\n</code></pre> <pre><code>def rerank(\n    query: str,\n    corpus: Sequence[str],\n    indexes: Sequence[int],\n    tokenizer,\n    reranker,\n    context_len: int,\n    device=None,\n):\n    if device is None:\n        device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n\n    pairs = [[query, corpus[i]] for i in indexes]\n    inputs = tokenizer(\n        pairs, padding=True, truncation=True, max_length=context_len, return_tensors=\"pt\"\n    ).to(device)\n\n    reranker = reranker.to(device).eval()\n    with tc.inference_mode():\n        logits = reranker(**inputs).logits.reshape(-1).cpu().numpy()\n\n    return np.array(indexes)[(-logits).argsort()]\n</code></pre> <p>First, issue-comment pairs of top-10 similarities are retrieved from the Faiss index. Then, the reranker reorders them in decreasing scores.</p> <pre><code>metrics, faiss_result_indexes = faiss_index.search(q_embeddings, k=10)\n</code></pre> <pre><code>reranked_indexes = np.array(\n    [\n        rerank(q, corpus, i, rerank_tokenizer, reranker, context_len=4096)\n        for q, i in zip(queries, faiss_result_indexes)\n    ]\n)\n</code></pre> <p>With our examples, the final top-1 results are quite similar. But the reranked results for question 1 and 3 are arguably more complete.</p> <pre><code>faiss_result_indexes\n</code></pre> <pre><code>array([[25352, 25339, 25344, 25340, 25351, 25350, 25345, 25343, 25346,\n        25341],\n       [ 2833,  2832, 22932, 25560, 20466, 25559, 20467, 20468, 20469,\n        20464],\n       [ 2230,  2231,  6440,  6458,  2229,  6457,  6455, 16575,  6453,\n         6437],\n       [18026, 18024, 18025, 23750,  4664, 12986, 13010, 16315,  4665,\n        13004],\n       [24623, 24624, 33204,  8035,  4781, 25094, 20774,  8034,  3762,\n        15749]])\n</code></pre> <pre><code>reranked_indexes\n</code></pre> <pre><code>array([[25339, 25352, 25340, 25344, 25341, 25350, 25351, 25346, 25345,\n        25343],\n       [ 2833,  2832, 25559, 20466, 25560, 20467, 20464, 22932, 20468,\n        20469],\n       [ 6458,  6440,  6453,  6457,  6455,  6437,  2229,  2230,  2231,\n        16575],\n       [18026, 18024, 18025,  4665,  4664, 23750, 16315, 12986, 13010,\n        13004],\n       [24623, 25094, 15749, 24624,  4781,  8034,  3762,  8035, 33204,\n        20774]])\n</code></pre> <pre><code>display_query_results(queries, corpus_df, reranked_indexes[:, 0].reshape(-1).tolist())\n</code></pre> <pre><code>Query: What is the difference between `uv pip install` and `uv add`?\n\nResult URL: https://github.com/astral-sh/uv/issues/9219#issuecomment-2485573603\n\nResult:\nIssue 9219: What's the difference between `uv pip install` and `uv add`\n\n[COMMENT]\nI've been using `uv` for a while and I really enjoy it. I keep stumbling onto the same confusion though. I never quite know whether do to:\n```sh\nuv init\nuv venv\nuv add polars marimo\nuv run hello.py\n```\nor\n```sh\nuv init\nuv venv\nsource .venv/bin/activate\npip install polars marimo\npython hello.py\n```\nare these two above equivalent?\n---\nalso are these two equivalent?\n```\nuv add polars\n```\n```\nuv pip install polars\n```\n\nComment:\n``uv add`` choose universal or cross-platform dependencies , and ``uv add`` is a project API.\nhttps://docs.astral.sh/uv/concepts/projects/\nThis is my understanding, but the more correct interpretation should be based on the documentation and the uv team's explanation.\n&gt; Suppose a dependency has versions 1.0.0 and 1.1.0 on Windows, but versions 1.0.0, 1.1.0, and 1.2.0 on Linux. If you're using uv on Linux, uv pip install would typically install the latest version (1.2.0), while uv add would select version 1.1.0, ensuring compatibility across Windows and Linux.\nThe explanation in the document should be here:\n* uv add\n&gt; uv's lockfile (uv.lock) is created with a universal resolution and is portable across platforms. This ensures that dependencies are locked for everyone working on the project, regardless of operating system, architecture, and Python version. The uv lockfile is created and modified by project commands such as uv lock, uv sync, and uv add.\nhttps://docs.astral.sh/uv/concepts/resolution/#universal-resolution\n* uv pip install\n&gt; By default, uv tries to use the latest version of each package. For example, uv pip install flask&gt;=2.0.0 will install the latest version of Flask, e.g., 3.0.0. If flask&gt;=2.0.0 is a dependency of the project, only flask 3.0.0 will be used. This is important, for example, because running tests will not check that the project is actually compatible with its stated lower bound of flask 2.0.0.\nhttps://docs.astral.sh/uv/concepts/resolution/#resolution-strategy\n\n\n==================================================\n\nQuery: How to update Python in my venv to the latest version?\n\nResult URL: https://github.com/astral-sh/uv/issues/11317#issuecomment-2643263625\n\nResult:\nIssue 11317: How to update `uv` managed Python version for applications?\n\n### Question\nI have Python installed via uv's `uv python install 3.x`. The project is an project/application (so not a library).\nLet's say there is a new Python (standalone) release, e.g.\n3.12.9 -&gt; 3.13.2\n3.13.1 -&gt; 3.13.2\nIt there a way to 'update' the standalone Python from uv, also replacing the existing `.venv`?\nMy current solution is to install the new python with `uv python install 3.x`, deleting the .venv and recreating it with the target python version (and delete the `.python-version` file). This works fine, but is a rather tedious process.\nI would love to have a project scoped command like `uv sync --update-python-to 3.13.2`  (handling also the installation of the standalone-python).\nSimilar possibilities:\n- `uv python update` (either updating existing standalone python versions on patch level AND/OR updating project .venv on python version patch level)\n- `uv python update 3.13` (updating project .venv to latest specified python version)\nI guess there are smarter persons, having better ideas how to handle these commands. Is there something in uv right now, that I'm misssing?\n### Platform\n_No response_\n### Version\n0.5.29\n\nComment:\nDang, didn't found that one. Thx!\n\n==================================================\n\nQuery: How to install the CPU version of PyTorch?\n\nResult URL: https://github.com/astral-sh/uv/issues/1497#issuecomment-2102236399\n\nResult:\nIssue 1497: Cannot install the CPU version of torch\n\nI tried to install the CPU version of torch but could not.\n```bash\nuv pip install torch==2.1.0+cpu --find-links https://download.pytorch.org/whl/torch_stable.html\n# and next command gives the same result.\nuv pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cpu\n```\n```log\n  \u00d7 No solution found when resolving dependencies:\n  \u2570\u2500\u25b6 Because there is no version of torch==2.1.0+cpu and you require torch==2.1.0+cpu, we can conclude that the requirements are unsatisfiable.\n```\nuv version: v0.1.2\nPython 3.11.7\nUbuntu 20.4\nX86_64 Architecture\n---\nBy the way, the install of the GPU version of torch is successful.\n```bash\nuv pip install torch==2.1.0\n# success\n```\n\nComment:\nPerhaps it's due to a lack of supported variants that they omitted the local identifier there? (_[here upstream PyTorch states they don't intend to publish `aarch64` variants with GPU accel](https://github.com/pytorch/pytorch/issues/110791#issuecomment-1753315240)_)\n```bash\n# Torch 2.3.0 + Python 3.12 Linux x86_64/aarch64 wheels\n# +cpu\n# https://download.pytorch.org/whl/cpu/torch/\ntorch-2.3.0+cpu-cp312-cp312-linux_x86_64.whl\n# No local identifier for arm64:\ntorch-2.3.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# +cu121\n# https://download.pytorch.org/whl/cu121/torch/\ntorch-2.3.0+cu121-cp312-cp312-linux_x86_64.whl\n```\n## Reference\nUsing Docker from an AMD64 machine to test ARM64 environment:\n```console\n# Base environment:\n$ arch\nx86_64\n$ docker run --rm -it --platform linux/arm64 --workdir /tmp fedora:40 bash\n$ arch\naarch64\n# Install uv:\n$ curl -LsSf https://astral.sh/uv/install.sh | sh &amp;&amp; source $HOME/.cargo/env\n# Create and enter venv:\n$ uv venv &amp;&amp; source .venv/bin/activate\n# Install:\n$ uv pip install \\\n    --extra-index-url https://download.pytorch.org/whl/cpu \\\n    torch torchvision torchaudio\nResolved 13 packages in 20.18s\nDownloaded 13 packages in 12.26s\nInstalled 13 packages in 486ms\n + filelock==3.13.1\n + fsspec==2024.2.0\n + jinja2==3.1.3\n + markupsafe==2.1.5\n + mpmath==1.3.0\n + networkx==3.2.1\n + numpy==1.26.3\n + pillow==10.2.0\n + sympy==1.12\n + torch==2.3.0\n + torchaudio==2.3.0\n + torchvision==0.18.0\n + typing-extensions==4.9.0\n```\n---\nYou can kind of see why PyTorch has local identifier variants when comparing to PyPi?:\n```console\n# ARM64 (PyPi instead of PyTorch)\n# Slightly newer versions of the packages but otherwise equivalent\n$ uv pip install torch torchvision torchaudio\nResolved 13 packages in 1.08s\nDownloaded 13 packages in 23.73s\nInstalled 13 packages in 425ms\n + filelock==3.14.0\n + fsspec==2024.3.1\n + jinja2==3.1.4\n + markupsafe==2.1.5\n + mpmath==1.3.0\n + networkx==3.3\n + numpy==1.26.4\n + pillow==10.3.0\n + sympy==1.12\n + torch==2.3.0\n + torchaudio==2.3.0\n + torchvision==0.18.0\n + typing-extensions==4.11.0\n```\n[CODE]\n---\n## Resolve platform packaging inconsistency upstream\nMaybe open an issue about it upstream since it seems to be more of a packaging concern unrelated to `uv`?:\nhttps://github.com/pytorch/pytorch/issues/110791#issuecomment-1753334468\n&gt; _please do not hesitate to open a new issue (or a PR) if you want to propose dropping `+cpu` suffix for Linux and Windows wheels, as nobody seems to be relying on the old installation method anyway._\nWhich makes sense that CPU should be the default, with local identifiers only used for actual variants (_eg: different cuda version support_). Might introduce some friction to any existing CI perhaps, but you can see similar with nvidia's own Docker images they publish where the naming convention isn't always consistent \ud83d\ude05 (_at least it broke a PyTorch docker build CI task recently_)\n\n==================================================\n\nQuery: Can I add a package dependency without version requirement?\n\nResult URL: https://github.com/astral-sh/uv/issues/6476#issuecomment-2305937941\n\nResult:\nIssue 6476: Allow adding a dependency with no version constraint\n\n`uv add` adds a lower bound version constraint by default. For example, calling `uv add requests` currently adds the dependency `\"requests&gt;=2.32.3\"` to `pyproject.toml`.\nIt's possible to have uv use different upper and/or lower bounds, but I could not find a way to add a dependency without any version constraint at all. For example, I want the dependency `\"requests\"` added to `pyproject.toml` without any bounds.\nThe current default behavior is fine, but I think there should be some way to add an unconstrained dependency, either with a global configuration setting that changes the default behavior, or with a command-line option for `uv add`.\nApologies if I missed something in the documentation and this is already possible. (Of course I could edit `pyproject.toml` manually, but it doesn't seem like that should be necessary.)\nThanks for the great work on a fantastic project!\n\nComment:\nThat seems ok to me.\n\n==================================================\n\nQuery: What does the `.python-version` file do?\n\nResult URL: https://github.com/astral-sh/uv/issues/8920#issuecomment-2465005344\n\nResult:\nIssue 8920: Purpose of .python-version?\n\n[COMMENT]\nCurrently on `uv init` a project boilerplate created includes the file `.python-version`.\nWhat is the purpose of this file, if there are constraints in `pyproject.toml` and `uv.lock`?\nIs it just for compatibility with tools like pyenv or is there more to it?\nI really like how clean project directory became after moving a lot of stuff (like `requirements.*`) to `pyproject.toml`. So wonder, if `.python-version` is really required in two senses:\n1. Is it required for the projects now? I tested by deleting `.python-version`, and everything works as expected and this file is not recreated with commands like `uv run`, `uv sync` and `uv lock`\n2. Since a lot of constraints/pinned versions of things are now in `pyproject.toml` and `uv.lock`, maybe if `.python-version` still plays some important role, its function can be moved to `pyproject.toml` or `uv.lock` in future?\nAsking as a 5-year user of `pyenv` and `.python-version`. They are great, but I don't really miss them.\nAlso, couldn't find peps related to this file.\n\nComment:\nRelated - https://github.com/astral-sh/uv/issues/8247\nA `.python-version` file is not strictly required, but it's useful to have it when developing a project as it allows you to specify the exact Python version you are using to do development. It's different to the `requires-python` field  - that is the range of Python versions supported by your project.\n\n\n==================================================\n</code></pre>","tags":["NLP","Polars","Hugging Face","Series: GitHub repo issues dataset"]},{"location":"setup-mkdocs-material-blog/","title":"Setting up a blog with Material for MkDocs","text":"<p>GitHub template:  dd-n-kk/mkdocs-material-blog</p> <p>\u200b</p> <p>I specify YAML settings with a JSON pointer-inspired syntax, using <code>i</code> as a wildcard array index.</p> <p>For example, <code>/theme/features/i: navigation.instant</code> means:</p> <pre><code>theme:\n  features:\n  - navigation.instant\n</code></pre>","tags":["Material for MkDocs"]},{"location":"setup-mkdocs-material-blog/#how-things-are-related","title":"How things are related","text":"<ul> <li> <p>MkDocs is a static documentation site generator based on   Python Markdown, and a core dependency of Material for MkDocs.</p> </li> <li> <p> Material for MkDocs was initially an MkDocs theme,   but has evolved into a full-fledged documentation framework. In particular,   it has many built-in plugins that provide opt-in website features.</p> </li> <li> <p>What Material for MkDocs refer to as extensions are   syntax extensions of Python Markdown. They are grouped into:</p> <ul> <li>Python Markdown: The built-in extensions of Python Markdown.</li> <li>Python Markdown extension:   These are PyMdown Extensions which further strengthens Python Markdown.</li> </ul> </li> <li> <p>mkdocstrings is an MkDocs plugin that provides powerful functionalities   such as auto-documentation and API cross-reference.   It is indispensable for documentation but out of scope for a blog.</p> </li> </ul>","tags":["Material for MkDocs"]},{"location":"setup-mkdocs-material-blog/#how-to-get-started-smoothly","title":"How to get started smoothly","text":"<ul> <li> <p>In MkDocs documentation:</p> <ul> <li>This page introduces some fundamental concepts and syntax.</li> <li>This page explains deployment. (Essentially, run <code>mkdocs gh-deploy</code>.)</li> </ul> </li> <li> <p>The documentation structure of Material for MkDocs is not super straightforward.   I suggest:</p> <ol> <li> <p>Skim through the following sections in Getting started:</p> <ul> <li>Installation</li> <li>Creating your site</li> <li>Customization</li> <li>Basic blogs</li> <li>Navigation, authors, and pagination</li> </ul> </li> <li> <p>Create a minimal setup of your blog.</p> </li> <li> <p>Go through Setup and adjust your <code>mkdocs.yml</code> accordingly.    But I suggest skipping Extensions for now.</p> </li> <li> <p>Go through Plugins and add what you need.</p> <ul> <li>Particularly: Blog, Meta, Search, and Tags.</li> </ul> </li> <li> <p>Go through Reference,    which is more of a goal-oriented guide about extensions.</p> <ul> <li>Particularly: Code blocks, Math, Images,   Footnotes, and Icons &amp; emojis.</li> <li>You might wanna just copy-paste the recommended setup.</li> </ul> </li> </ol> </li> </ul>","tags":["Material for MkDocs"]},{"location":"setup-mkdocs-material-blog/#navigation-in-plain-words","title":"Navigation in plain words","text":"<p>\u200b</p> <p>All options in this section are under <code>/theme/features/i:</code>.</p> <ul> <li> <p>By default, the left sidebar handles all cross-page navigation,   with subsections folded within top-level sections.</p> </li> <li> <p><code>navigation.tabs</code>:</p> <p>The top-level sections are moved into a horizonal menu below the header. Second-level sections become \"top-level\" in the left sidebar. Usually this is tidy, but for a shallow-structured site such as a blog, this may cause the left sidebar to be empty and the section title to be duplicated in the horizontal menu, the left sidebar, and the main content area.</p> </li> <li> <p><code>navigation.sections</code>:</p> <p>The effective top-level sections in the left sidebar are unfolded if the viewport is large enough.</p> </li> <li> <p><code>navigation.expand</code>:</p> <p>All subsections in the left sidebar are expanded.</p> </li> <li> <p><code>navigation.indexes</code>:</p> <p>If a section is structured as a directory with an <code>index.md</code>, in the left sidebar the link to <code>index</code> is merged with the section title.</p> </li> <li> <p><code>navigation.prune</code>:</p> <p>This option avoids rendering folded subsection titles with a trick: When clicking to \"unfold\" a section, you are actually directed into the section page. Naturally, this is incompatible with <code>navigation.expand</code>.</p> </li> <li> <p><code>navigation.instant</code>:</p> <p>This option enables single-page application mode which improves load time. However, I haved observed at least two ill side effects:</p> <ul> <li>Plotly plots may require a manual refresh to load properly.</li> <li>giscus comment system may not change theme when light/dark mode is toggled.</li> </ul> </li> </ul>","tags":["Material for MkDocs"]},{"location":"setup-mkdocs-material-blog/#tips-and-gotchas","title":"Tips and gotchas","text":"<ul> <li> <p>Python Markdown requires each nesting level to be indented by 4 spaces.</p> </li> <li> <p>Add <code>site/</code> to <code>.gitignore</code> in the main branch   because its contents are generated by <code>mkdocs build</code>.</p> </li> <li> <p>For a stand-alone blog (one that is not part of a documentation site),   set <code>/plugins/i/blog/blog_dir: .</code> so that the blog is placed at root level.</p> </li> <li> <p><code>/plugins/i/tags/tags_file</code> is deprecated.   Use a tags listing marker instead.</p> </li> <li> <p>When <code>/theme/palette/primary</code> is set to be a neutral color (e.g., <code>black</code>),   link text will be colored indigo. This can be overriden in the custom CSS:   <pre><code>[data-md-color-scheme=\"default\"][data-md-color-primary=\"black\"] { /* Light mode */\n  --md-typeset-a-color: #002577;\n}\n[data-md-color-scheme=\"slate\"][data-md-color-primary=\"black\"] { /* Dark mode */\n  --md-typeset-a-color: #E6CA95;\n}\n</code></pre></p> </li> <li> <p><code>sane_lists</code> is another useful extension not mentioned by the documentation     of Material for MkDocs.</p> </li> </ul>","tags":["Material for MkDocs"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/data-preparation/","title":"Data preparation","text":""},{"location":"category/deep-learning/","title":"Deep learning","text":""},{"location":"category/miscellany/","title":"Miscellany","text":""}]}